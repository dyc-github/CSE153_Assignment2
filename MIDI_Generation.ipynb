{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyc-github/CSE153_Assignment2/blob/main/MIDI_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Ecs-6yxiud"
      },
      "source": [
        "# Install and load required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8g_o3ViHxiue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde64eda-e9d3-4a31-d5d4-6705a1e76d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting miditok\n",
            "  Downloading miditok-3.0.5.post1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from miditok) (2.0.2)\n",
            "Collecting symusic>=0.5.0 (from miditok)\n",
            "  Downloading symusic-0.5.8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from miditok) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (1.1.2)\n",
            "Collecting pySmartDL (from symusic>=0.5.0->miditok)\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from symusic>=0.5.0->miditok) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2025.4.26)\n",
            "Downloading miditok-3.0.5.post1-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symusic-0.5.8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pySmartDL, symusic, miditok\n",
            "Successfully installed miditok-3.0.5.post1 pySmartDL-1.3.4 symusic-0.5.8\n",
            "Collecting mido\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido) (24.2)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mido\n",
            "Successfully installed mido-1.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install miditok\n",
        "!pip install mido\n",
        "#!pip install symusic\n",
        "#!pip install glob\n",
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data.zip"
      ],
      "metadata": {
        "id": "dl0QrrmN2E1K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g4VJpcg4xiue"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from typing import List\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from mido import MidiFile\n",
        "from symusic import Score\n",
        "from miditok import REMI, TokenizerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "8z0dRTAL3Q8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "58o78Ax9xiug"
      },
      "outputs": [],
      "source": [
        "train_files = glob.glob(\"./data/train/*.mid\")\n",
        "test_files = glob.glob(\"./data/test/*.mid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_midi_len(file):\n",
        "  try:\n",
        "    mid = MidiFile(file)\n",
        "    return mid.length\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "p0OC4igS6KCO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(train_files)\n",
        "print(f\"Number of training samples: {num_samples}\")\n",
        "total_length = sum([get_midi_len(file) for file in train_files])\n",
        "print(f\"Total length of training samples in ticks: {total_length}\")\n",
        "avg_length = total_length / num_samples\n",
        "print(f\"Avg length of training samples in ticks: {avg_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak0SBU1_3kcQ",
        "outputId": "1127588e-7eb4-40f5-bb9b-fee0fb5a608e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 638\n",
            "Total length of training samples in ticks: 72914.93901779644\n",
            "Avg length of training samples in ticks: 114.28673827240821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzLU_2qjxiug"
      },
      "source": [
        "# Model: Second Order Markov Chain Model\n",
        "This model serves as a baseline of comparison for our LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "WTYrgv2k4LI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Midi Tokenizer"
      ],
      "metadata": {
        "id": "cB7ScuK8S_Fm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i-fRQcSyxiug",
        "outputId": "0bdc74e5-0510-41e1-b915-61e58609b3cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
            "  super().__init__(tokenizer_config, params)\n"
          ]
        }
      ],
      "source": [
        "config = TokenizerConfig(num_velocities=1, use_chords=False, use_programs=True)\n",
        "tokenizer = REMI(config)\n",
        "tokenizer.train(vocab_size=1000, files_paths=train_files)\n",
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct PyTorch Dataset and Dataloaders"
      ],
      "metadata": {
        "id": "puCAVcrhTH5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3XTgwwDbxiug"
      },
      "outputs": [],
      "source": [
        "class MIDIDataset(Dataset):\n",
        "    def __init__(self, file_paths: List[str], tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_paths = file_paths\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        midi = Score(self.file_paths[idx])\n",
        "        tokens = self.tokenizer(midi)\n",
        "        return np.array(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZpF66QOKxiug"
      },
      "outputs": [],
      "source": [
        "train_dataset = MIDIDataset(train_files, tokenizer)\n",
        "test_dataset = MIDIDataset(test_files, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1lYNgFYxiug"
      },
      "source": [
        "## Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wbbkhpLZxiug"
      },
      "outputs": [],
      "source": [
        "class SecondOrderMarkovChain:\n",
        "    def __init__(self):\n",
        "        self.transitions = defaultdict(lambda: defaultdict(int))\n",
        "        self.probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        for sequence in train_loader:\n",
        "            sequence = sequence[0].numpy().astype(int)\n",
        "            for i in range(len(sequence) - 2):\n",
        "                state1, state2 = sequence[i], sequence[i + 1]\n",
        "                next_state = sequence[i + 2]\n",
        "                self.transitions[(state1, state2)][next_state] += 1\n",
        "\n",
        "        for (state1, state2), next_states in self.transitions.items():\n",
        "            total = sum(next_states.values())\n",
        "            for next_state, count in next_states.items():\n",
        "                self.probabilities[(state1, state2)][next_state] = count / total\n",
        "        return self.probabilities\n",
        "\n",
        "    def generate(self, test_sequence, num_predictions=1):\n",
        "        test_sequence = test_sequence[0].numpy().astype(int)\n",
        "        results = [test_sequence[0], test_sequence[1]]\n",
        "        for i in range(100):\n",
        "            if (results[-2], results[-1]) not in self.probabilities:\n",
        "                break\n",
        "            else:\n",
        "                probs = self.probabilities[(results[-2], results[-1])]\n",
        "                states = list(probs.keys())\n",
        "                probabilities = list(probs.values())\n",
        "                if not states:\n",
        "                    break\n",
        "                try:\n",
        "                    predictions = np.random.choice(states, size=num_predictions, p=probabilities)\n",
        "                except:\n",
        "                    break\n",
        "                results.append(predictions[0])\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzKyEWGNxiug"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbLz6RjZxiug"
      },
      "outputs": [],
      "source": [
        "model = SecondOrderMarkovChain()\n",
        "model.train(train_loader)\n",
        "\n",
        "predictions = []\n",
        "for test_sequence in test_loader:\n",
        "    predictions.append(model.generate(test_sequence))\n",
        "for i, prediction in enumerate(predictions):\n",
        "    output_score = tokenizer.decode(torch.Tensor(prediction))\n",
        "    output_score.dump_midi(f\"markov/{i}.mid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Output"
      ],
      "metadata": {
        "id": "oqcLde6Jrq5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r markov.zip ./markov\n",
        "files.download(\"markov.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-gAwYnJtrpqF",
        "outputId": "d7a3c782-7970-45db-a036-f96062b95195"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: markov/ (stored 0%)\n",
            "  adding: markov/28.mid (deflated 41%)\n",
            "  adding: markov/57.mid (deflated 40%)\n",
            "  adding: markov/66.mid (stored 0%)\n",
            "  adding: markov/58.mid (deflated 39%)\n",
            "  adding: markov/69.mid (deflated 35%)\n",
            "  adding: markov/15.mid (deflated 41%)\n",
            "  adding: markov/31.mid (deflated 42%)\n",
            "  adding: markov/48.mid (stored 0%)\n",
            "  adding: markov/12.mid (deflated 36%)\n",
            "  adding: markov/24.mid (deflated 39%)\n",
            "  adding: markov/43.mid (deflated 39%)\n",
            "  adding: markov/30.mid (deflated 39%)\n",
            "  adding: markov/39.mid (deflated 38%)\n",
            "  adding: markov/27.mid (deflated 41%)\n",
            "  adding: markov/38.mid (deflated 38%)\n",
            "  adding: markov/60.mid (deflated 33%)\n",
            "  adding: markov/9.mid (deflated 37%)\n",
            "  adding: markov/44.mid (deflated 2%)\n",
            "  adding: markov/20.mid (deflated 36%)\n",
            "  adding: markov/49.mid (stored 0%)\n",
            "  adding: markov/37.mid (deflated 40%)\n",
            "  adding: markov/10.mid (deflated 38%)\n",
            "  adding: markov/41.mid (deflated 39%)\n",
            "  adding: markov/7.mid (deflated 40%)\n",
            "  adding: markov/2.mid (deflated 39%)\n",
            "  adding: markov/42.mid (deflated 40%)\n",
            "  adding: markov/11.mid (deflated 37%)\n",
            "  adding: markov/33.mid (deflated 38%)\n",
            "  adding: markov/6.mid (deflated 38%)\n",
            "  adding: markov/23.mid (deflated 37%)\n",
            "  adding: markov/4.mid (deflated 38%)\n",
            "  adding: markov/17.mid (deflated 39%)\n",
            "  adding: markov/59.mid (deflated 37%)\n",
            "  adding: markov/45.mid (deflated 39%)\n",
            "  adding: markov/16.mid (stored 0%)\n",
            "  adding: markov/34.mid (deflated 34%)\n",
            "  adding: markov/46.mid (stored 0%)\n",
            "  adding: markov/65.mid (deflated 42%)\n",
            "  adding: markov/54.mid (deflated 41%)\n",
            "  adding: markov/47.mid (deflated 34%)\n",
            "  adding: markov/52.mid (deflated 35%)\n",
            "  adding: markov/25.mid (deflated 40%)\n",
            "  adding: markov/8.mid (deflated 40%)\n",
            "  adding: markov/32.mid (deflated 39%)\n",
            "  adding: markov/36.mid (deflated 34%)\n",
            "  adding: markov/63.mid (deflated 42%)\n",
            "  adding: markov/0.mid (deflated 35%)\n",
            "  adding: markov/51.mid (deflated 38%)\n",
            "  adding: markov/5.mid (deflated 37%)\n",
            "  adding: markov/67.mid (deflated 39%)\n",
            "  adding: markov/29.mid (deflated 37%)\n",
            "  adding: markov/26.mid (deflated 41%)\n",
            "  adding: markov/61.mid (deflated 38%)\n",
            "  adding: markov/22.mid (deflated 40%)\n",
            "  adding: markov/50.mid (deflated 38%)\n",
            "  adding: markov/1.mid (deflated 45%)\n",
            "  adding: markov/3.mid (deflated 39%)\n",
            "  adding: markov/18.mid (deflated 2%)\n",
            "  adding: markov/64.mid (deflated 41%)\n",
            "  adding: markov/62.mid (stored 0%)\n",
            "  adding: markov/13.mid (deflated 40%)\n",
            "  adding: markov/55.mid (deflated 33%)\n",
            "  adding: markov/53.mid (deflated 37%)\n",
            "  adding: markov/56.mid (deflated 37%)\n",
            "  adding: markov/21.mid (deflated 40%)\n",
            "  adding: markov/40.mid (deflated 39%)\n",
            "  adding: markov/35.mid (deflated 38%)\n",
            "  adding: markov/19.mid (deflated 33%)\n",
            "  adding: markov/68.mid (deflated 40%)\n",
            "  adding: markov/14.mid (deflated 39%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b31b715e-a11a-4544-8fb4-4d64dfb29a90\", \"markov.zip\", 30909)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: LSTM Model\n",
        "This is the primary model I will be exploring"
      ],
      "metadata": {
        "id": "VBN0aYQ3TsIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "V2w9rx8CX7SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
        "\n",
        "tokenizer = REMI()  # using defaults parameters (constants.py)\n",
        "train_dataset = DatasetMIDI(\n",
        "    files_paths=train_files,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=1024,\n",
        "    bos_token_id=tokenizer[\"BOS_None\"],\n",
        "    eos_token_id=tokenizer[\"EOS_None\"],\n",
        ")\n",
        "test_dataset = DatasetMIDI(\n",
        "    files_paths=test_files,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=1024,\n",
        "    bos_token_id=tokenizer[\"BOS_None\"],\n",
        "    eos_token_id=tokenizer[\"EOS_None\"],\n",
        ")\n",
        "collator = DataCollator(tokenizer.pad_token_id)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
      ],
      "metadata": {
        "id": "PtVV2bvtkNZQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN62DQ2MbnvF",
        "outputId": "05444237-6089-48d6-97f0-244757e81311"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "VR7hZbrLTyu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super(MusicRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch_size, seq_length)\n",
        "        x = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        out, hidden = self.rnn(x, hidden)  # out: (batch_size, seq_length, hidden_dim)\n",
        "        out = self.fc(out)  # (batch_size, seq_length, vocab_size)\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "wfddI849TzsN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "aVSszSART00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --------- Training ---------\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            outputs = outputs.reshape(-1, vocab_size)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # --------- Validation ---------\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch['input_ids'].to(device)\n",
        "\n",
        "                inputs = batch[:, :-1]\n",
        "                targets = batch[:, 1:]\n",
        "\n",
        "                outputs, _ = model(inputs)\n",
        "                outputs = outputs.reshape(-1, vocab_size)\n",
        "                targets = targets.reshape(-1)\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    embedding_dim = 256\n",
        "    hidden_dim = 512\n",
        "    num_layers = 2\n",
        "    dropout = .3\n",
        "\n",
        "    model = MusicRNN(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
        "    train(model, train_loader, test_loader, vocab_size, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OelLTyQT3gY",
        "outputId": "52e919b0-783e-4731-ca06-33f3de30db06"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Train Loss: 2.7964 | Val Loss: 1.9718\n",
            "Epoch 2/15 | Train Loss: 1.7968 | Val Loss: 1.6201\n",
            "Epoch 3/15 | Train Loss: 1.5021 | Val Loss: 1.4396\n",
            "Epoch 4/15 | Train Loss: 1.3268 | Val Loss: 1.3043\n",
            "Epoch 5/15 | Train Loss: 1.2244 | Val Loss: 1.2479\n",
            "Epoch 6/15 | Train Loss: 1.1488 | Val Loss: 1.2160\n",
            "Epoch 7/15 | Train Loss: 1.0998 | Val Loss: 1.1965\n",
            "Epoch 8/15 | Train Loss: 1.0568 | Val Loss: 1.1939\n",
            "Epoch 9/15 | Train Loss: 1.0244 | Val Loss: 1.1704\n",
            "Epoch 10/15 | Train Loss: 0.9711 | Val Loss: 1.1626\n",
            "Epoch 11/15 | Train Loss: 0.9323 | Val Loss: 1.1570\n",
            "Epoch 12/15 | Train Loss: 0.8970 | Val Loss: 1.1454\n",
            "Epoch 13/15 | Train Loss: 0.8553 | Val Loss: 1.1402\n",
            "Epoch 14/15 | Train Loss: 0.8196 | Val Loss: 1.1539\n",
            "Epoch 15/15 | Train Loss: 0.7933 | Val Loss: 1.1445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 15 Epoch Model"
      ],
      "metadata": {
        "id": "v0NQxTiiT3-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, start_token, max_length=100, temperature=1.0, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    generated = [start_token]\n",
        "    input_token = torch.tensor([[start_token]], device=device)  # (1, 1)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(input_token, hidden)  # output: (1, 1, vocab_size)\n",
        "        output = output[:, -1, :]  # take the last output\n",
        "        output = output / temperature  # adjust randomness\n",
        "\n",
        "        probs = F.softmax(output, dim=-1)  # (1, vocab_size)\n",
        "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated.append(next_token)\n",
        "        if next_token == 2 or next_token == 0: # reach end of sequence\n",
        "          break\n",
        "\n",
        "        input_token = torch.tensor([[next_token]], device=device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "start_token = tokenizer.special_tokens_ids[1]\n",
        "generated_sequence = sample(model, start_token, max_length=1024)\n",
        "\n",
        "print(\"Generated token sequence:\")\n",
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqGTcuhtT7c6",
        "outputId": "29db49b0-4f00-44e4-eafd-8cc14e9da3d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated token sequence:\n",
            "[1, 4, 189, 44, 124, 132, 197, 49, 124, 128, 201, 47, 124, 128, 205, 51, 124, 128, 209, 49, 124, 132, 217, 44, 124, 126, 219, 44, 124, 126, 4, 189, 42, 124, 128, 193, 47, 124, 128, 197, 46, 124, 136, 209, 51, 124, 128, 213, 47, 124, 128, 217, 47, 124, 128, 4, 189, 42, 124, 132, 197, 40, 124, 132, 205, 40, 124, 132, 213, 37, 124, 132, 4, 189, 38, 124, 126, 191, 34, 124, 126, 193, 35, 124, 126, 195, 30, 124, 126, 197, 18, 124, 140, 213, 30, 124, 126, 217, 43, 124, 126, 219, 42, 124, 126, 4, 189, 40, 124, 140, 47, 124, 140, 205, 41, 124, 140, 42, 124, 140, 4, 189, 66, 124, 140, 38, 124, 138, 205, 40, 124, 140, 44, 124, 140, 4, 189, 53, 124, 156, 4, 189, 44, 124, 156, 4, 189, 44, 124, 126, 191, 42, 124, 126, 193, 41, 124, 126, 195, 37, 124, 126, 197, 35, 124, 126, 199, 37, 124, 126, 201, 38, 124, 126, 203, 38, 124, 126, 205, 30, 124, 126, 207, 42, 124, 126, 209, 46, 124, 126, 211, 46, 124, 126, 213, 25, 124, 126, 215, 46, 124, 126, 217, 42, 124, 126, 219, 42, 124, 126, 4, 189, 40, 124, 126, 191, 43, 124, 126, 193, 40, 124, 126, 195, 42, 124, 126, 201, 44, 124, 126, 203, 47, 124, 126, 205, 44, 124, 126, 207, 48, 124, 126, 209, 54, 124, 126, 211, 52, 124, 126, 213, 50, 124, 126, 215, 46, 124, 126, 217, 47, 124, 126, 219, 49, 124, 126, 4, 189, 47, 124, 126, 191, 45, 124, 126, 193, 49, 124, 126, 195, 47, 124, 126, 197, 116, 124, 126, 199, 47, 124, 126, 201, 44, 124, 126, 203, 42, 124, 126, 205, 47, 124, 126, 207, 52, 124, 126, 209, 54, 124, 126, 211, 44, 124, 126, 213, 47, 124, 132, 4, 189, 46, 124, 132, 197, 49, 124, 126, 199, 47, 124, 126, 201, 49, 124, 126, 203, 52, 124, 126, 207, 45, 124, 126, 209, 47, 124, 126, 211, 47, 124, 126, 213, 51, 124, 126, 215, 50, 124, 126, 217, 46, 124, 126, 219, 45, 124, 126, 4, 189, 45, 124, 128, 193, 43, 124, 126, 219, 38, 124, 126, 205, 42, 124, 126, 207, 42, 124, 126, 209, 47, 124, 128, 213, 41, 124, 132, 4, 189, 35, 124, 126, 191, 40, 124, 126, 193, 47, 124, 126, 195, 39, 124, 126, 197, 40, 124, 140, 213, 50, 124, 132, 4, 193, 43, 124, 126, 195, 42, 124, 126, 197, 45, 124, 126, 199, 43, 124, 126, 201, 41, 124, 126, 203, 40, 124, 126, 205, 45, 124, 126, 207, 49, 124, 126, 209, 39, 124, 126, 211, 45, 124, 126, 213, 43, 124, 126, 215, 45, 124, 126, 217, 44, 124, 126, 219, 47, 124, 126, 4, 189, 41, 124, 132, 197, 45, 124, 132, 205, 44, 124, 132, 213, 40, 124, 132, 4, 189, 43, 124, 132, 197, 42, 124, 132, 205, 44, 124, 132, 4, 189, 44, 124, 140, 205, 46, 124, 140, 4, 189, 47, 124, 140, 205, 47, 124, 140, 4, 189, 49, 124, 140, 205, 37, 124, 140, 4, 189, 35, 124, 140, 205, 37, 124, 140, 4, 189, 37, 124, 140, 4, 4, 189, 63, 124, 132, 56, 124, 140, 197, 59, 124, 132, 205, 59, 124, 132, 58, 124, 132, 213, 59, 124, 132, 61, 124, 132, 4, 189, 63, 124, 126, 67, 124, 126, 191, 66, 124, 126, 59, 124, 126, 193, 61, 124, 126, 66, 124, 126, 195, 64, 124, 126, 64, 124, 126, 199, 66, 124, 126, 66, 124, 126, 201, 63, 124, 132, 52, 124, 132, 46, 124, 132, 205, 66, 124, 132, 209, 66, 124, 126, 74, 124, 126, 211, 66, 124, 126, 66, 124, 126, 213, 75, 124, 126, 215, 66, 124, 126, 78, 124, 126, 217, 66, 124, 126, 66, 124, 126, 219, 65, 124, 126, 71, 124, 126, 4, 189, 66, 124, 126, 71, 124, 126, 193, 71, 124, 126, 63, 124, 126, 195, 68, 124, 126, 69, 124, 126, 197, 66, 124, 126, 64, 124, 126, 199, 69, 124, 126, 59, 124, 126, 201, 68, 123, 126, 69, 124, 126, 203, 64, 124, 126, 71, 124, 126, 205, 66, 124, 126, 66, 124, 126, 207, 65, 124, 126, 68, 124, 126, 209, 69, 124, 126, 64, 124, 126, 211, 68, 124, 126, 65, 124, 126, 215, 63, 124, 126, 66, 124, 126, 217, 61, 124, 126, 4, 191, 67, 124, 126, 72, 124, 126, 193, 73, 124, 126, 124, 124, 126, 195, 64, 124, 126, 57, 124, 126, 197, 71, 124, 126, 55, 124, 126, 199, 59, 124, 126, 56, 124, 126, 201, 64, 124, 126, 65, 124, 126, 203, 60, 124, 126, 69, 124, 126, 205, 66, 124, 126, 69, 124, 126, 207, 62, 124, 126, 59, 124, 126, 209, 65, 124, 126, 64, 124, 126, 211, 59, 124, 126, 213, 64, 124, 126, 69, 124, 126, 215, 67, 124, 126, 217, 65, 124, 126, 57, 124, 126, 219, 58, 124, 126, 62, 124, 126, 4, 189, 48, 124, 126, 52, 124, 126, 191, 55, 124, 126, 55, 124, 126, 193, 52, 124, 126, 57, 124, 126, 195, 52, 124, 126, 53, 124, 126, 197, 57, 124, 126, 57, 124, 126, 199, 52, 124, 126, 57, 124, 126, 201, 53, 124, 126, 57, 124, 126, 203, 57, 124, 126, 59, 124, 126, 205, 55, 124, 126, 57, 124, 126, 207, 55, 124, 126, 57, 124, 126, 209, 57, 124, 126, 55, 124, 126, 211, 56, 124, 128, 57, 124, 126, 215, 52, 124, 126, 57, 124, 126, 4, 189, 53, 124, 136, 57, 124, 140, 56, 124, 130, 201, 56, 124, 132, 61, 124, 132, 209, 51, 124, 128, 54, 124, 128, 213, 53, 124, 132, 56, 124, 132, 4, 189, 59, 124, 128, 56, 124, 128, 193, 52, 124, 128, 54, 124, 128, 57, 124, 128, 197, 59, 124, 128, 51, 124, 128, 201, 51, 124, 128, 61, 124, 128, 205, 58, 124, 128, 53, 124, 128, 61, 124, 128, 209, 56, 124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_15/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO1oUMnSaVeL",
        "outputId": "055107e8-9f26-4d54-96dd-34c2f6b56031"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-2ec83c333e95>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r rnn_15.zip ./rnn_15\n",
        "files.download(\"rnn_15.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "tO185dV9anMm",
        "outputId": "d7c95dad-d46d-4eb8-c711-3161256b2675"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: rnn_15/ (stored 0%)\n",
            "  adding: rnn_15/rnn_5.mid (deflated 61%)\n",
            "  adding: rnn_15/rnn_0.mid (deflated 59%)\n",
            "  adding: rnn_15/rnn_1.mid (deflated 52%)\n",
            "  adding: rnn_15/rnn_3.mid (deflated 67%)\n",
            "  adding: rnn_15/rnn_2.mid (deflated 52%)\n",
            "  adding: rnn_15/rnn_6.mid (deflated 54%)\n",
            "  adding: rnn_15/rnn_4.mid (deflated 65%)\n",
            "  adding: rnn_15/rnn_7.mid (deflated 51%)\n",
            "  adding: rnn_15/rnn_8.mid (deflated 2%)\n",
            "  adding: rnn_15/rnn_9.mid (deflated 50%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71967b45-9e22-4d4e-9ba8-76615acc9cf7\", \"rnn_15.zip\", 9211)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 25 Epoch Model"
      ],
      "metadata": {
        "id": "zUfcRl36sJYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader, vocab_size, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cDH_USvq4f4",
        "outputId": "6878921a-bc2d-4c91-ffca-316578034f4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 0.7644 | Val Loss: 1.1469\n",
            "Epoch 2/10 | Train Loss: 0.7250 | Val Loss: 1.1617\n",
            "Epoch 3/10 | Train Loss: 0.6925 | Val Loss: 1.1655\n",
            "Epoch 4/10 | Train Loss: 0.6596 | Val Loss: 1.1808\n",
            "Epoch 5/10 | Train Loss: 0.6267 | Val Loss: 1.1996\n",
            "Epoch 6/10 | Train Loss: 0.6004 | Val Loss: 1.2031\n",
            "Epoch 7/10 | Train Loss: 0.5703 | Val Loss: 1.2232\n",
            "Epoch 8/10 | Train Loss: 0.5399 | Val Loss: 1.2488\n",
            "Epoch 9/10 | Train Loss: 0.5151 | Val Loss: 1.2462\n",
            "Epoch 10/10 | Train Loss: 0.4844 | Val Loss: 1.2806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_25/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc5dCk7ksGeb",
        "outputId": "dba48358-954a-412b-e072-3c61c089432d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-b088643153c2>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r rnn_25.zip ./rnn_25\n",
        "files.download(\"rnn_25.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "T0TvH2utmJXa",
        "outputId": "86b6b540-c93f-40a9-e78b-da7a9aad3edf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: rnn_25/ (stored 0%)\n",
            "  adding: rnn_25/rnn_5.mid (deflated 71%)\n",
            "  adding: rnn_25/rnn_0.mid (deflated 2%)\n",
            "  adding: rnn_25/rnn_1.mid (deflated 61%)\n",
            "  adding: rnn_25/rnn_3.mid (deflated 48%)\n",
            "  adding: rnn_25/rnn_2.mid (deflated 64%)\n",
            "  adding: rnn_25/rnn_6.mid (deflated 75%)\n",
            "  adding: rnn_25/rnn_4.mid (deflated 68%)\n",
            "  adding: rnn_25/rnn_7.mid (deflated 71%)\n",
            "  adding: rnn_25/rnn_8.mid (deflated 65%)\n",
            "  adding: rnn_25/rnn_9.mid (deflated 75%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80c2ad4d-8d48-45d2-97c6-bdf3eac1ce0a\", \"rnn_25.zip\", 6936)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 50 Epoch Model\n"
      ],
      "metadata": {
        "id": "ZyeVQyuWsnhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader, vocab_size, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C632s42tBEU",
        "outputId": "4fb76f74-e8c6-440d-9445-7564b66ac89f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Train Loss: 0.4660 | Val Loss: 1.3109\n",
            "Epoch 2/25 | Train Loss: 0.4301 | Val Loss: 1.3256\n",
            "Epoch 3/25 | Train Loss: 0.4093 | Val Loss: 1.3385\n",
            "Epoch 4/25 | Train Loss: 0.3873 | Val Loss: 1.3756\n",
            "Epoch 5/25 | Train Loss: 0.3738 | Val Loss: 1.4017\n",
            "Epoch 6/25 | Train Loss: 0.3531 | Val Loss: 1.4187\n",
            "Epoch 7/25 | Train Loss: 0.3376 | Val Loss: 1.4391\n",
            "Epoch 8/25 | Train Loss: 0.3207 | Val Loss: 1.4662\n",
            "Epoch 9/25 | Train Loss: 0.3036 | Val Loss: 1.4819\n",
            "Epoch 10/25 | Train Loss: 0.2872 | Val Loss: 1.5067\n",
            "Epoch 11/25 | Train Loss: 0.2718 | Val Loss: 1.5175\n",
            "Epoch 12/25 | Train Loss: 0.2532 | Val Loss: 1.5516\n",
            "Epoch 13/25 | Train Loss: 0.2442 | Val Loss: 1.5718\n",
            "Epoch 14/25 | Train Loss: 0.2355 | Val Loss: 1.5953\n",
            "Epoch 15/25 | Train Loss: 0.2303 | Val Loss: 1.6487\n",
            "Epoch 16/25 | Train Loss: 0.2297 | Val Loss: 1.6254\n",
            "Epoch 17/25 | Train Loss: 0.2108 | Val Loss: 1.6725\n",
            "Epoch 18/25 | Train Loss: 0.1975 | Val Loss: 1.7077\n",
            "Epoch 19/25 | Train Loss: 0.1855 | Val Loss: 1.7225\n",
            "Epoch 20/25 | Train Loss: 0.1789 | Val Loss: 1.7439\n",
            "Epoch 21/25 | Train Loss: 0.1782 | Val Loss: 1.7471\n",
            "Epoch 22/25 | Train Loss: 0.1725 | Val Loss: 1.7761\n",
            "Epoch 23/25 | Train Loss: 0.1651 | Val Loss: 1.7997\n",
            "Epoch 24/25 | Train Loss: 0.1592 | Val Loss: 1.8070\n",
            "Epoch 25/25 | Train Loss: 0.1531 | Val Loss: 1.8271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_50/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvCcVSDtFhD",
        "outputId": "0b8d8ecd-3b2b-4c82-bc4b-af5961598db6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ce489eda5a3f>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r rnn_50.zip ./rnn_50\n",
        "files.download(\"rnn_50.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "BXCRcJY9tGsG",
        "outputId": "e292d4e8-d1f6-4029-dadf-897ef8b23868"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: rnn_50/ (stored 0%)\n",
            "  adding: rnn_50/rnn_5.mid (deflated 60%)\n",
            "  adding: rnn_50/rnn_0.mid (deflated 67%)\n",
            "  adding: rnn_50/rnn_1.mid (deflated 63%)\n",
            "  adding: rnn_50/rnn_3.mid (deflated 59%)\n",
            "  adding: rnn_50/rnn_2.mid (deflated 79%)\n",
            "  adding: rnn_50/rnn_6.mid (deflated 66%)\n",
            "  adding: rnn_50/rnn_4.mid (deflated 64%)\n",
            "  adding: rnn_50/rnn_7.mid (deflated 62%)\n",
            "  adding: rnn_50/rnn_8.mid (deflated 68%)\n",
            "  adding: rnn_50/rnn_9.mid (deflated 70%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80709751-39d8-4b49-b612-625132fed072\", \"rnn_50.zip\", 8239)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Crd5LcK_vdtT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
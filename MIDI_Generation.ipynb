{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyc-github/CSE153_Assignment2/blob/main/MIDI_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Ecs-6yxiud"
      },
      "source": [
        "# Install and load required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8g_o3ViHxiue"
      },
      "outputs": [],
      "source": [
        "# !pip install miditok\n",
        "# !pip install mido\n",
        "#!pip install symusic\n",
        "#!pip install glob\n",
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data.zip"
      ],
      "metadata": {
        "id": "dl0QrrmN2E1K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g4VJpcg4xiue"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "import glob\n",
        "import random\n",
        "from typing import List\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from mido import MidiFile\n",
        "from symusic import Score\n",
        "from miditok import REMI, TokenizerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "8z0dRTAL3Q8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "58o78Ax9xiug"
      },
      "outputs": [],
      "source": [
        "train_files = glob.glob(\"./data/train/*.mid\")\n",
        "test_files = glob.glob(\"./data/test/*.mid\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_midi_len(file):\n",
        "  try:\n",
        "    mid = MidiFile(file)\n",
        "    return mid.length\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return 0"
      ],
      "metadata": {
        "id": "p0OC4igS6KCO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(train_files)\n",
        "print(f\"Number of training samples: {num_samples}\")\n",
        "total_length = sum([get_midi_len(file) for file in train_files])\n",
        "print(f\"Total length of training samples in ticks: {total_length}\")\n",
        "avg_length = total_length / num_samples\n",
        "print(f\"Avg length of training samples in ticks: {avg_length}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak0SBU1_3kcQ",
        "outputId": "1127588e-7eb4-40f5-bb9b-fee0fb5a608e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 638\n",
            "Total length of training samples in ticks: 72914.93901779644\n",
            "Avg length of training samples in ticks: 114.28673827240821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzLU_2qjxiug"
      },
      "source": [
        "# Model: Second Order Markov Chain Model\n",
        "This model serves as a baseline of comparison for our LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "WTYrgv2k4LI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Midi Tokenizer"
      ],
      "metadata": {
        "id": "cB7ScuK8S_Fm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i-fRQcSyxiug",
        "outputId": "0bdc74e5-0510-41e1-b915-61e58609b3cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/miditok/tokenizations/remi.py:88: UserWarning: Attribute controls are not compatible with 'config.one_token_stream_for_programs' and multi-vocabulary tokenizers. Disabling them from the config.\n",
            "  super().__init__(tokenizer_config, params)\n"
          ]
        }
      ],
      "source": [
        "config = TokenizerConfig(num_velocities=1, use_chords=False, use_programs=True)\n",
        "tokenizer = REMI(config)\n",
        "tokenizer.train(vocab_size=1000, files_paths=train_files)\n",
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct PyTorch Dataset and Dataloaders"
      ],
      "metadata": {
        "id": "puCAVcrhTH5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3XTgwwDbxiug"
      },
      "outputs": [],
      "source": [
        "class MIDIDataset(Dataset):\n",
        "    def __init__(self, file_paths: List[str], tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_paths = file_paths\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        midi = Score(self.file_paths[idx])\n",
        "        tokens = self.tokenizer(midi)\n",
        "        return np.array(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZpF66QOKxiug"
      },
      "outputs": [],
      "source": [
        "train_dataset = MIDIDataset(train_files, tokenizer)\n",
        "test_dataset = MIDIDataset(test_files, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1lYNgFYxiug"
      },
      "source": [
        "## Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wbbkhpLZxiug"
      },
      "outputs": [],
      "source": [
        "class SecondOrderMarkovChain:\n",
        "    def __init__(self):\n",
        "        self.transitions = defaultdict(lambda: defaultdict(int))\n",
        "        self.probabilities = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        for sequence in train_loader:\n",
        "            sequence = sequence[0].numpy().astype(int)\n",
        "            for i in range(len(sequence) - 2):\n",
        "                state1, state2 = sequence[i], sequence[i + 1]\n",
        "                next_state = sequence[i + 2]\n",
        "                self.transitions[(state1, state2)][next_state] += 1\n",
        "\n",
        "        for (state1, state2), next_states in self.transitions.items():\n",
        "            total = sum(next_states.values())\n",
        "            for next_state, count in next_states.items():\n",
        "                self.probabilities[(state1, state2)][next_state] = count / total\n",
        "        return self.probabilities\n",
        "\n",
        "    def generate(self, test_sequence, num_predictions=1):\n",
        "        test_sequence = test_sequence[0].numpy().astype(int)\n",
        "        results = [test_sequence[0], test_sequence[1]]\n",
        "        for i in range(100):\n",
        "            if (results[-2], results[-1]) not in self.probabilities:\n",
        "                break\n",
        "            else:\n",
        "                probs = self.probabilities[(results[-2], results[-1])]\n",
        "                states = list(probs.keys())\n",
        "                probabilities = list(probs.values())\n",
        "                if not states:\n",
        "                    break\n",
        "                try:\n",
        "                    predictions = np.random.choice(states, size=num_predictions, p=probabilities)\n",
        "                except:\n",
        "                    break\n",
        "                results.append(predictions[0])\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzKyEWGNxiug"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbLz6RjZxiug"
      },
      "outputs": [],
      "source": [
        "model = SecondOrderMarkovChain()\n",
        "model.train(train_loader)\n",
        "\n",
        "predictions = []\n",
        "for test_sequence in test_loader:\n",
        "    predictions.append(model.generate(test_sequence))\n",
        "for i, prediction in enumerate(predictions):\n",
        "    output_score = tokenizer.decode(torch.Tensor(prediction))\n",
        "    output_score.dump_midi(f\"markov/{i}.mid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Output"
      ],
      "metadata": {
        "id": "oqcLde6Jrq5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r markov.zip ./markov\n",
        "# files.download(\"markov.zip\")"
      ],
      "metadata": {
        "id": "-gAwYnJtrpqF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model: LSTM Model\n",
        "This is the primary model I will be exploring"
      ],
      "metadata": {
        "id": "VBN0aYQ3TsIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "V2w9rx8CX7SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
        "\n",
        "tokenizer = REMI()  # using defaults parameters (constants.py)\n",
        "train_dataset = DatasetMIDI(\n",
        "    files_paths=train_files,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=1024,\n",
        "    bos_token_id=tokenizer[\"BOS_None\"],\n",
        "    eos_token_id=tokenizer[\"EOS_None\"],\n",
        ")\n",
        "test_dataset = DatasetMIDI(\n",
        "    files_paths=test_files,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=1024,\n",
        "    bos_token_id=tokenizer[\"BOS_None\"],\n",
        "    eos_token_id=tokenizer[\"EOS_None\"],\n",
        ")\n",
        "collator = DataCollator(tokenizer.pad_token_id)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)"
      ],
      "metadata": {
        "id": "PtVV2bvtkNZQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN62DQ2MbnvF",
        "outputId": "05444237-6089-48d6-97f0-244757e81311"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "VR7hZbrLTyu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super(MusicRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch_size, seq_length)\n",
        "        x = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
        "        out, hidden = self.rnn(x, hidden)  # out: (batch_size, seq_length, hidden_dim)\n",
        "        out = self.fc(out)  # (batch_size, seq_length, vocab_size)\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "wfddI849TzsN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "aVSszSART00u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, vocab_size, num_epochs=20, lr=0.001, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # --------- Training ---------\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(inputs)\n",
        "            outputs = outputs.reshape(-1, vocab_size)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # --------- Validation ---------\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch['input_ids'].to(device)\n",
        "\n",
        "                inputs = batch[:, :-1]\n",
        "                targets = batch[:, 1:]\n",
        "\n",
        "                outputs, _ = model(inputs)\n",
        "                outputs = outputs.reshape(-1, vocab_size)\n",
        "                targets = targets.reshape(-1)\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = tokenizer.vocab_size\n",
        "    embedding_dim = 256\n",
        "    hidden_dim = 512\n",
        "    num_layers = 2\n",
        "    dropout = .3\n",
        "\n",
        "    model = MusicRNN(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
        "    train(model, train_loader, test_loader, vocab_size, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OelLTyQT3gY",
        "outputId": "52e919b0-783e-4731-ca06-33f3de30db06"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 | Train Loss: 2.7964 | Val Loss: 1.9718\n",
            "Epoch 2/15 | Train Loss: 1.7968 | Val Loss: 1.6201\n",
            "Epoch 3/15 | Train Loss: 1.5021 | Val Loss: 1.4396\n",
            "Epoch 4/15 | Train Loss: 1.3268 | Val Loss: 1.3043\n",
            "Epoch 5/15 | Train Loss: 1.2244 | Val Loss: 1.2479\n",
            "Epoch 6/15 | Train Loss: 1.1488 | Val Loss: 1.2160\n",
            "Epoch 7/15 | Train Loss: 1.0998 | Val Loss: 1.1965\n",
            "Epoch 8/15 | Train Loss: 1.0568 | Val Loss: 1.1939\n",
            "Epoch 9/15 | Train Loss: 1.0244 | Val Loss: 1.1704\n",
            "Epoch 10/15 | Train Loss: 0.9711 | Val Loss: 1.1626\n",
            "Epoch 11/15 | Train Loss: 0.9323 | Val Loss: 1.1570\n",
            "Epoch 12/15 | Train Loss: 0.8970 | Val Loss: 1.1454\n",
            "Epoch 13/15 | Train Loss: 0.8553 | Val Loss: 1.1402\n",
            "Epoch 14/15 | Train Loss: 0.8196 | Val Loss: 1.1539\n",
            "Epoch 15/15 | Train Loss: 0.7933 | Val Loss: 1.1445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 15 Epoch Model"
      ],
      "metadata": {
        "id": "v0NQxTiiT3-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, start_token, max_length=100, temperature=1.0, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    generated = [start_token]\n",
        "    input_token = torch.tensor([[start_token]], device=device)  # (1, 1)\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(input_token, hidden)  # output: (1, 1, vocab_size)\n",
        "        output = output[:, -1, :]  # take the last output\n",
        "        output = output / temperature  # adjust randomness\n",
        "\n",
        "        probs = F.softmax(output, dim=-1)  # (1, vocab_size)\n",
        "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated.append(next_token)\n",
        "        if next_token == 2 or next_token == 0: # reach end of sequence\n",
        "          break\n",
        "\n",
        "        input_token = torch.tensor([[next_token]], device=device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "start_token = tokenizer.special_tokens_ids[1]\n",
        "generated_sequence = sample(model, start_token, max_length=1024)\n",
        "\n",
        "print(\"Generated token sequence:\")\n",
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqGTcuhtT7c6",
        "outputId": "29db49b0-4f00-44e4-eafd-8cc14e9da3d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated token sequence:\n",
            "[1, 4, 189, 44, 124, 132, 197, 49, 124, 128, 201, 47, 124, 128, 205, 51, 124, 128, 209, 49, 124, 132, 217, 44, 124, 126, 219, 44, 124, 126, 4, 189, 42, 124, 128, 193, 47, 124, 128, 197, 46, 124, 136, 209, 51, 124, 128, 213, 47, 124, 128, 217, 47, 124, 128, 4, 189, 42, 124, 132, 197, 40, 124, 132, 205, 40, 124, 132, 213, 37, 124, 132, 4, 189, 38, 124, 126, 191, 34, 124, 126, 193, 35, 124, 126, 195, 30, 124, 126, 197, 18, 124, 140, 213, 30, 124, 126, 217, 43, 124, 126, 219, 42, 124, 126, 4, 189, 40, 124, 140, 47, 124, 140, 205, 41, 124, 140, 42, 124, 140, 4, 189, 66, 124, 140, 38, 124, 138, 205, 40, 124, 140, 44, 124, 140, 4, 189, 53, 124, 156, 4, 189, 44, 124, 156, 4, 189, 44, 124, 126, 191, 42, 124, 126, 193, 41, 124, 126, 195, 37, 124, 126, 197, 35, 124, 126, 199, 37, 124, 126, 201, 38, 124, 126, 203, 38, 124, 126, 205, 30, 124, 126, 207, 42, 124, 126, 209, 46, 124, 126, 211, 46, 124, 126, 213, 25, 124, 126, 215, 46, 124, 126, 217, 42, 124, 126, 219, 42, 124, 126, 4, 189, 40, 124, 126, 191, 43, 124, 126, 193, 40, 124, 126, 195, 42, 124, 126, 201, 44, 124, 126, 203, 47, 124, 126, 205, 44, 124, 126, 207, 48, 124, 126, 209, 54, 124, 126, 211, 52, 124, 126, 213, 50, 124, 126, 215, 46, 124, 126, 217, 47, 124, 126, 219, 49, 124, 126, 4, 189, 47, 124, 126, 191, 45, 124, 126, 193, 49, 124, 126, 195, 47, 124, 126, 197, 116, 124, 126, 199, 47, 124, 126, 201, 44, 124, 126, 203, 42, 124, 126, 205, 47, 124, 126, 207, 52, 124, 126, 209, 54, 124, 126, 211, 44, 124, 126, 213, 47, 124, 132, 4, 189, 46, 124, 132, 197, 49, 124, 126, 199, 47, 124, 126, 201, 49, 124, 126, 203, 52, 124, 126, 207, 45, 124, 126, 209, 47, 124, 126, 211, 47, 124, 126, 213, 51, 124, 126, 215, 50, 124, 126, 217, 46, 124, 126, 219, 45, 124, 126, 4, 189, 45, 124, 128, 193, 43, 124, 126, 219, 38, 124, 126, 205, 42, 124, 126, 207, 42, 124, 126, 209, 47, 124, 128, 213, 41, 124, 132, 4, 189, 35, 124, 126, 191, 40, 124, 126, 193, 47, 124, 126, 195, 39, 124, 126, 197, 40, 124, 140, 213, 50, 124, 132, 4, 193, 43, 124, 126, 195, 42, 124, 126, 197, 45, 124, 126, 199, 43, 124, 126, 201, 41, 124, 126, 203, 40, 124, 126, 205, 45, 124, 126, 207, 49, 124, 126, 209, 39, 124, 126, 211, 45, 124, 126, 213, 43, 124, 126, 215, 45, 124, 126, 217, 44, 124, 126, 219, 47, 124, 126, 4, 189, 41, 124, 132, 197, 45, 124, 132, 205, 44, 124, 132, 213, 40, 124, 132, 4, 189, 43, 124, 132, 197, 42, 124, 132, 205, 44, 124, 132, 4, 189, 44, 124, 140, 205, 46, 124, 140, 4, 189, 47, 124, 140, 205, 47, 124, 140, 4, 189, 49, 124, 140, 205, 37, 124, 140, 4, 189, 35, 124, 140, 205, 37, 124, 140, 4, 189, 37, 124, 140, 4, 4, 189, 63, 124, 132, 56, 124, 140, 197, 59, 124, 132, 205, 59, 124, 132, 58, 124, 132, 213, 59, 124, 132, 61, 124, 132, 4, 189, 63, 124, 126, 67, 124, 126, 191, 66, 124, 126, 59, 124, 126, 193, 61, 124, 126, 66, 124, 126, 195, 64, 124, 126, 64, 124, 126, 199, 66, 124, 126, 66, 124, 126, 201, 63, 124, 132, 52, 124, 132, 46, 124, 132, 205, 66, 124, 132, 209, 66, 124, 126, 74, 124, 126, 211, 66, 124, 126, 66, 124, 126, 213, 75, 124, 126, 215, 66, 124, 126, 78, 124, 126, 217, 66, 124, 126, 66, 124, 126, 219, 65, 124, 126, 71, 124, 126, 4, 189, 66, 124, 126, 71, 124, 126, 193, 71, 124, 126, 63, 124, 126, 195, 68, 124, 126, 69, 124, 126, 197, 66, 124, 126, 64, 124, 126, 199, 69, 124, 126, 59, 124, 126, 201, 68, 123, 126, 69, 124, 126, 203, 64, 124, 126, 71, 124, 126, 205, 66, 124, 126, 66, 124, 126, 207, 65, 124, 126, 68, 124, 126, 209, 69, 124, 126, 64, 124, 126, 211, 68, 124, 126, 65, 124, 126, 215, 63, 124, 126, 66, 124, 126, 217, 61, 124, 126, 4, 191, 67, 124, 126, 72, 124, 126, 193, 73, 124, 126, 124, 124, 126, 195, 64, 124, 126, 57, 124, 126, 197, 71, 124, 126, 55, 124, 126, 199, 59, 124, 126, 56, 124, 126, 201, 64, 124, 126, 65, 124, 126, 203, 60, 124, 126, 69, 124, 126, 205, 66, 124, 126, 69, 124, 126, 207, 62, 124, 126, 59, 124, 126, 209, 65, 124, 126, 64, 124, 126, 211, 59, 124, 126, 213, 64, 124, 126, 69, 124, 126, 215, 67, 124, 126, 217, 65, 124, 126, 57, 124, 126, 219, 58, 124, 126, 62, 124, 126, 4, 189, 48, 124, 126, 52, 124, 126, 191, 55, 124, 126, 55, 124, 126, 193, 52, 124, 126, 57, 124, 126, 195, 52, 124, 126, 53, 124, 126, 197, 57, 124, 126, 57, 124, 126, 199, 52, 124, 126, 57, 124, 126, 201, 53, 124, 126, 57, 124, 126, 203, 57, 124, 126, 59, 124, 126, 205, 55, 124, 126, 57, 124, 126, 207, 55, 124, 126, 57, 124, 126, 209, 57, 124, 126, 55, 124, 126, 211, 56, 124, 128, 57, 124, 126, 215, 52, 124, 126, 57, 124, 126, 4, 189, 53, 124, 136, 57, 124, 140, 56, 124, 130, 201, 56, 124, 132, 61, 124, 132, 209, 51, 124, 128, 54, 124, 128, 213, 53, 124, 132, 56, 124, 132, 4, 189, 59, 124, 128, 56, 124, 128, 193, 52, 124, 128, 54, 124, 128, 57, 124, 128, 197, 59, 124, 128, 51, 124, 128, 201, 51, 124, 128, 61, 124, 128, 205, 58, 124, 128, 53, 124, 128, 61, 124, 128, 209, 56, 124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_15/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO1oUMnSaVeL",
        "outputId": "055107e8-9f26-4d54-96dd-34c2f6b56031"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-2ec83c333e95>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r rnn_15.zip ./rnn_15\n",
        "# files.download(\"rnn_15.zip\")"
      ],
      "metadata": {
        "id": "tO185dV9anMm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 25 Epoch Model"
      ],
      "metadata": {
        "id": "zUfcRl36sJYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader, vocab_size, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cDH_USvq4f4",
        "outputId": "6878921a-bc2d-4c91-ffca-316578034f4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 0.7644 | Val Loss: 1.1469\n",
            "Epoch 2/10 | Train Loss: 0.7250 | Val Loss: 1.1617\n",
            "Epoch 3/10 | Train Loss: 0.6925 | Val Loss: 1.1655\n",
            "Epoch 4/10 | Train Loss: 0.6596 | Val Loss: 1.1808\n",
            "Epoch 5/10 | Train Loss: 0.6267 | Val Loss: 1.1996\n",
            "Epoch 6/10 | Train Loss: 0.6004 | Val Loss: 1.2031\n",
            "Epoch 7/10 | Train Loss: 0.5703 | Val Loss: 1.2232\n",
            "Epoch 8/10 | Train Loss: 0.5399 | Val Loss: 1.2488\n",
            "Epoch 9/10 | Train Loss: 0.5151 | Val Loss: 1.2462\n",
            "Epoch 10/10 | Train Loss: 0.4844 | Val Loss: 1.2806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_25/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc5dCk7ksGeb",
        "outputId": "dba48358-954a-412b-e072-3c61c089432d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-b088643153c2>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r rnn_25.zip ./rnn_25\n",
        "# files.download(\"rnn_25.zip\")"
      ],
      "metadata": {
        "id": "T0TvH2utmJXa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train 50 Epoch Model\n"
      ],
      "metadata": {
        "id": "ZyeVQyuWsnhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader, vocab_size, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C632s42tBEU",
        "outputId": "4fb76f74-e8c6-440d-9445-7564b66ac89f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Train Loss: 0.4660 | Val Loss: 1.3109\n",
            "Epoch 2/25 | Train Loss: 0.4301 | Val Loss: 1.3256\n",
            "Epoch 3/25 | Train Loss: 0.4093 | Val Loss: 1.3385\n",
            "Epoch 4/25 | Train Loss: 0.3873 | Val Loss: 1.3756\n",
            "Epoch 5/25 | Train Loss: 0.3738 | Val Loss: 1.4017\n",
            "Epoch 6/25 | Train Loss: 0.3531 | Val Loss: 1.4187\n",
            "Epoch 7/25 | Train Loss: 0.3376 | Val Loss: 1.4391\n",
            "Epoch 8/25 | Train Loss: 0.3207 | Val Loss: 1.4662\n",
            "Epoch 9/25 | Train Loss: 0.3036 | Val Loss: 1.4819\n",
            "Epoch 10/25 | Train Loss: 0.2872 | Val Loss: 1.5067\n",
            "Epoch 11/25 | Train Loss: 0.2718 | Val Loss: 1.5175\n",
            "Epoch 12/25 | Train Loss: 0.2532 | Val Loss: 1.5516\n",
            "Epoch 13/25 | Train Loss: 0.2442 | Val Loss: 1.5718\n",
            "Epoch 14/25 | Train Loss: 0.2355 | Val Loss: 1.5953\n",
            "Epoch 15/25 | Train Loss: 0.2303 | Val Loss: 1.6487\n",
            "Epoch 16/25 | Train Loss: 0.2297 | Val Loss: 1.6254\n",
            "Epoch 17/25 | Train Loss: 0.2108 | Val Loss: 1.6725\n",
            "Epoch 18/25 | Train Loss: 0.1975 | Val Loss: 1.7077\n",
            "Epoch 19/25 | Train Loss: 0.1855 | Val Loss: 1.7225\n",
            "Epoch 20/25 | Train Loss: 0.1789 | Val Loss: 1.7439\n",
            "Epoch 21/25 | Train Loss: 0.1782 | Val Loss: 1.7471\n",
            "Epoch 22/25 | Train Loss: 0.1725 | Val Loss: 1.7761\n",
            "Epoch 23/25 | Train Loss: 0.1651 | Val Loss: 1.7997\n",
            "Epoch 24/25 | Train Loss: 0.1592 | Val Loss: 1.8070\n",
            "Epoch 25/25 | Train Loss: 0.1531 | Val Loss: 1.8271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, start_token in enumerate(random.sample(list(tokenizer.vocab.values()), 10)):\n",
        "  generated_sequence = sample(model, start_token, max_length=1024)\n",
        "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n",
        "  output_score.dump_midi(f\"rnn_50/rnn_{i}.mid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvCcVSDtFhD",
        "outputId": "0b8d8ecd-3b2b-4c82-bc4b-af5961598db6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-ce489eda5a3f>:3: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
            "  output_score = tokenizer.tokens_to_midi([generated_sequence])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r rnn_50.zip ./rnn_50\n",
        "files.download(\"rnn_50.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "BXCRcJY9tGsG",
        "outputId": "e292d4e8-d1f6-4029-dadf-897ef8b23868"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: rnn_50/ (stored 0%)\n",
            "  adding: rnn_50/rnn_5.mid (deflated 60%)\n",
            "  adding: rnn_50/rnn_0.mid (deflated 67%)\n",
            "  adding: rnn_50/rnn_1.mid (deflated 63%)\n",
            "  adding: rnn_50/rnn_3.mid (deflated 59%)\n",
            "  adding: rnn_50/rnn_2.mid (deflated 79%)\n",
            "  adding: rnn_50/rnn_6.mid (deflated 66%)\n",
            "  adding: rnn_50/rnn_4.mid (deflated 64%)\n",
            "  adding: rnn_50/rnn_7.mid (deflated 62%)\n",
            "  adding: rnn_50/rnn_8.mid (deflated 68%)\n",
            "  adding: rnn_50/rnn_9.mid (deflated 70%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80709751-39d8-4b49-b612-625132fed072\", \"rnn_50.zip\", 8239)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Crd5LcK_vdtT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}